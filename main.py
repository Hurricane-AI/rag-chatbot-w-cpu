import streamlit as st
import ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

from transformers import pipeline

# Helsinki-NLPã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªã‚’è‹±èªã«ç¿»è¨³
translator_to_en = pipeline("translation", model="Helsinki-NLP/opus-mt-ja-en")
print(translator_to_en("åˆã‚ã¾ã—ã¦ã€ã“ã‚“ã«ã¡ã¯ã€‚")[0]["translation_text"])

translator_to_ja = pipeline("translation", model="staka/fugumt-en-ja")

st.title("Webãƒšãƒ¼ã‚¸ã¨ã®ãƒãƒ£ãƒƒãƒˆ ğŸŒ")
st.caption("ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®Llama-3ã¨RAGã‚’ä½¿ç”¨ã—ã¦Webãƒšãƒ¼ã‚¸ã¨ãƒãƒ£ãƒƒãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰Webãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—
webpage_url = st.text_input("Webãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="default")

if webpage_url:
    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    loader = WebBaseLoader(webpage_url)
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=10)
    splits = text_splitter.split_documents(docs)
    
    # 2. Ollamaã®åŸ‹ã‚è¾¼ã¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    embeddings = OllamaEmbeddings(model="hf.co/lm-kit/bge-m3-gguf:Q5_K_M")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    
    # 3. Ollama Llama3ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™
    def ollama_llm(question, context):
        formatted_prompt = f'You are an assistant for answering questions. Use the provided context to generate accurate and concise responses. If the context does not contain sufficient information, respond with "I do not know." Do not fabricate answers.\n\n### Question: {translator_to_en(question)[0]["translation_text"]}\n\n### Context: {context}\n\n### Answer:'
        print(formatted_prompt)
        response = ollama.chat(model='hf.co/lmstudio-community/Llama-3.3-70B-Instruct-GGUF:Q4_K_M', messages=[{'role': 'user', 'content': formatted_prompt}])
        return response['message']['content']
    
    # 4. RAGã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    retriever = vectorstore.as_retriever()

    def combine_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    def rag_chain(question):
        retrieved_docs = retriever.invoke(question)
        formatted_context = combine_docs(retrieved_docs)
        return ollama_llm(question, formatted_context)

    st.success(f"{webpage_url}ã‚’æ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼")
    
    # Webãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦è³ªå•ã™ã‚‹
    prompt = st.text_input("Webãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦ã®è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # Webãƒšãƒ¼ã‚¸ã¨ãƒãƒ£ãƒƒãƒˆã™ã‚‹
    if prompt:
        result = rag_chain(prompt)
        st.write(result)
        st.write(translator_to_ja(result)[0]["translation_text"])